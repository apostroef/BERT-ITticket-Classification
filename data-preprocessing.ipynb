{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b645b2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Library import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d8fff6",
   "metadata": {},
   "source": [
    "## **Data Load**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78e359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"translated_data.csv\")\n",
    "tiket_df = pd.DataFrame(df)\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ebbc1a",
   "metadata": {},
   "source": [
    "## **`Data Presentation`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fe45ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "priority = pd.DataFrame(tiket_df[\"priority\"])\n",
    "priority_order = df[\"priority\"].value_counts().index\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "a = sns.countplot(\n",
    "    data=df,\n",
    "    x=\"priority\",\n",
    "    hue=\"priority\",\n",
    "    order=priority_order,\n",
    "    palette=sns.color_palette(\"mako\"),\n",
    ")\n",
    "for container in a.containers:\n",
    "    a.bar_label(container, fmt=\"%d\", label_type=\"edge\", padding=5)\n",
    "plt.title(\"Priority Distribution\")\n",
    "plt.xlabel(\"Priority\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "# type\n",
    "type = pd.DataFrame(tiket_df[\"type\"])\n",
    "type_order = df[\"type\"].value_counts().index\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "a = sns.countplot(\n",
    "    data=df,\n",
    "    x=\"type\",\n",
    "    hue=\"type\",\n",
    "    order=type_order,\n",
    "    palette=sns.color_palette(\"mako\"),\n",
    ")\n",
    "for container in a.containers:\n",
    "    a.bar_label(container, fmt=\"%d\", label_type=\"edge\", padding=5)\n",
    "plt.title(\"Type Distribution\")\n",
    "plt.xlabel(\"Type\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "# queue\n",
    "queue = pd.DataFrame(tiket_df[\"queue\"])\n",
    "queue_order = df[\"queue\"].value_counts().index\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "a = sns.countplot(\n",
    "    data=df,\n",
    "    x=\"queue\",\n",
    "    hue=\"queue\",\n",
    "    order=queue_order,\n",
    "    palette=sns.color_palette(\"mako\"),\n",
    ")\n",
    "for container in a.containers:\n",
    "    a.bar_label(container, fmt=\"%d\", label_type=\"edge\", padding=5)\n",
    "plt.title(\"queue Distribution\")\n",
    "plt.xlabel(\"queue\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be625c44",
   "metadata": {},
   "source": [
    "## **`Data Splitting`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad3bbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 1. Load and prepare your data\n",
    "# -------------------------------\n",
    "\n",
    "# Encode categorical labels to numeric form\n",
    "tiket_df[\"priority_cat\"] = tiket_df[\"priority\"].astype(\"category\").cat.codes\n",
    "tiket_df[\"type_cat\"] = tiket_df[\"type\"].astype(\"category\").cat.codes\n",
    "tiket_df[\"queue_cat\"] = tiket_df[\"queue\"].astype(\"category\").cat.codes\n",
    "\n",
    "# Store the label mappings (for decoding later if needed)\n",
    "priority_classes = tiket_df[\"priority\"].astype(\"category\").cat.categories\n",
    "type_classes = tiket_df[\"type\"].astype(\"category\").cat.categories\n",
    "queue_classes = tiket_df[\"queue\"].astype(\"category\").cat.categories\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Split dataset\n",
    "# -------------------------------\n",
    "# Here we stratify using one target (priority) to keep distribution balanced\n",
    "X = tiket_df[\"translated_body\"]\n",
    "Y = tiket_df[[\"priority_cat\", \"type_cat\", \"queue_cat\"]].values\n",
    "\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42, stratify=Y[:, 0]  # stratify by priority\n",
    ")\n",
    "\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(\n",
    "    X_temp, Y_temp, test_size=0.5, random_state=42, stratify=Y_temp[:, 0]\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3697cda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert arrays back to DataFrames\n",
    "train_df = pd.DataFrame(Y_train, columns=[\"priority_cat\", \"type_cat\", \"queue_cat\"])\n",
    "val_df = pd.DataFrame(Y_val, columns=[\"priority_cat\", \"type_cat\", \"queue_cat\"])\n",
    "test_df = pd.DataFrame(Y_test, columns=[\"priority_cat\", \"type_cat\", \"queue_cat\"])\n",
    "\n",
    "\n",
    "def show_distribution(name, train, val, test, class_names):\n",
    "    print(f\"\\nðŸ“Š Distribution for target: {name}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    train_dist = pd.Series(train).value_counts(normalize=True).sort_index() * 100\n",
    "    val_dist = pd.Series(val).value_counts(normalize=True).sort_index() * 100\n",
    "    test_dist = pd.Series(test).value_counts(normalize=True).sort_index() * 100\n",
    "\n",
    "    dist_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Class\": class_names,\n",
    "            \"Train (%)\": train_dist.round(2).values,\n",
    "            \"Val (%)\": val_dist.round(2).values,\n",
    "            \"Test (%)\": test_dist.round(2).values,\n",
    "        }\n",
    "    ).fillna(0)\n",
    "\n",
    "    display(dist_df)\n",
    "\n",
    "\n",
    "# Show for all three targets\n",
    "show_distribution(\n",
    "    \"Priority\",\n",
    "    train_df[\"priority_cat\"],\n",
    "    val_df[\"priority_cat\"],\n",
    "    test_df[\"priority_cat\"],\n",
    "    priority_classes,\n",
    ")\n",
    "show_distribution(\n",
    "    \"Type\", train_df[\"type_cat\"], val_df[\"type_cat\"], test_df[\"type_cat\"], type_classes\n",
    ")\n",
    "show_distribution(\n",
    "    \"Queue\",\n",
    "    train_df[\"queue_cat\"],\n",
    "    val_df[\"queue_cat\"],\n",
    "    test_df[\"queue_cat\"],\n",
    "    queue_classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba5e7bf",
   "metadata": {},
   "source": [
    "# **`Tokenization`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da86d84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 3. Tokenize only the \"body\" column\n",
    "# -------------------------------\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_texts(texts):\n",
    "    texts = [str(t) if not pd.isna(t) else \"\" for t in texts]\n",
    "    return tokenizer(\n",
    "        texts, truncation=True, padding=True, max_length=256, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "\n",
    "X_train_tokenized = tokenize_texts(X_train)\n",
    "X_val_tokenized = tokenize_texts(X_val)\n",
    "X_test_tokenized = tokenize_texts(X_test)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Prepare label tensors (for each target)\n",
    "# -------------------------------\n",
    "y_train_priority = torch.tensor(Y_train[:, 0])\n",
    "y_train_type = torch.tensor(Y_train[:, 1])\n",
    "y_train_queue = torch.tensor(Y_train[:, 2])\n",
    "\n",
    "y_val_priority = torch.tensor(Y_val[:, 0])\n",
    "y_val_type = torch.tensor(Y_val[:, 1])\n",
    "y_val_queue = torch.tensor(Y_val[:, 2])\n",
    "\n",
    "y_test_priority = torch.tensor(Y_test[:, 0])\n",
    "y_test_type = torch.tensor(Y_test[:, 1])\n",
    "y_test_queue = torch.tensor(Y_test[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aab6f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 5. Wrap in dicts for dataloaders\n",
    "# -------------------------------\n",
    "train_data = {\n",
    "    \"input_ids\": X_train_tokenized[\"input_ids\"],\n",
    "    \"attention_mask\": X_train_tokenized[\"attention_mask\"],\n",
    "    \"priority\": y_train_priority,\n",
    "    \"type\": y_train_type,\n",
    "    \"queue\": y_train_queue,\n",
    "}\n",
    "\n",
    "\n",
    "val_data = {\n",
    "    \"input_ids\": X_val_tokenized[\"input_ids\"],\n",
    "    \"attention_mask\": X_val_tokenized[\"attention_mask\"],\n",
    "    \"priority\": y_val_priority,\n",
    "    \"type\": y_val_type,\n",
    "    \"queue\": y_val_queue,\n",
    "}\n",
    "\n",
    "test_data = {\n",
    "    \"input_ids\": X_test_tokenized[\"input_ids\"],\n",
    "    \"attention_mask\": X_test_tokenized[\"attention_mask\"],\n",
    "    \"priority\": y_test_priority,\n",
    "    \"type\": y_test_type,\n",
    "    \"queue\": y_test_queue,\n",
    "}\n",
    "torch.save(train_data, \"train_data.pt\")\n",
    "torch.save(val_data, \"validation_data.pt\")\n",
    "torch.save(test_data, \"test_data.pt\")\n",
    "\n",
    "print(train_data[\"input_ids\"].shape)\n",
    "print(f\"Train size: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67044ee",
   "metadata": {},
   "source": [
    "### decoded preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daa94fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_texts = tokenizer.batch_decode(\n",
    "    train_data[\"input_ids\"][:5], skip_special_tokens=False\n",
    ")\n",
    "\n",
    "# Build a DataFrame for human-readable inspection\n",
    "train_preview = pd.DataFrame(\n",
    "    {\n",
    "        \"decoded_body\": decoded_texts,\n",
    "        \"input_ids\": train_data[\"input_ids\"][:5].tolist(),\n",
    "        \"attention_mask\": train_data[\"attention_mask\"][:5].tolist(),\n",
    "        \"priority\": train_data[\"priority\"][:5].tolist(),\n",
    "        \"type\": train_data[\"type\"][:5].tolist(),\n",
    "        \"queue\": train_data[\"queue\"][:5].tolist(),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display neatly\n",
    "pd.set_option(\"max_colwidth\", 80)\n",
    "display(train_preview.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a789bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check imbalance ratio\n",
    "def imbalance_ratio(series):\n",
    "    counts = series.value_counts()\n",
    "    return counts.max() / counts.min()\n",
    "\n",
    "\n",
    "targets = [\"priority_cat\", \"type_cat\", \"queue_cat\"]\n",
    "\n",
    "for t in targets:\n",
    "    ratio = imbalance_ratio(train_df[t])\n",
    "    print(f\"{t.capitalize()} imbalance ratio: {ratio:.2f}\")\n",
    "    print(train_df[t].value_counts(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73964ea",
   "metadata": {},
   "source": [
    "# **Class Weighting**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfd1ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t class weights\n",
    "type_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(y_train_type),\n",
    "    y=y_train_type.numpy(),\n",
    ")\n",
    "type_weights = torch.tensor(type_weights, dtype=torch.float)\n",
    "\n",
    "# Queue class weights\n",
    "queue_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\", classes=np.unique(y_train_queue), y=y_train_queue.numpy()\n",
    ")\n",
    "queue_weights = torch.tensor(queue_weights, dtype=torch.float)\n",
    "\n",
    "print(\"Type class weights:\", type_weights)\n",
    "print(\"Queue class weights:\", queue_weights)\n",
    "\n",
    "# saving value\n",
    "type_weights = torch.tensor(type_weights, dtype=torch.float)\n",
    "queue_weights = torch.tensor(queue_weights, dtype=torch.float)\n",
    "torch.save(type_weights, \"type_weights.pt\")\n",
    "torch.save(queue_weights, \"queue_weights.pt\")\n",
    "\n",
    "# --- Display neatly ---\n",
    "type_weight_df = pd.DataFrame(\n",
    "    {\"Priority_Class\": type_classes, \"Weight\": type_weights.numpy()}\n",
    ")\n",
    "\n",
    "queue_weight_df = pd.DataFrame(\n",
    "    {\"Queue_Class\": queue_classes, \"Weight\": queue_weights.numpy()}\n",
    ")\n",
    "\n",
    "print(\"Priority Class Weights\")\n",
    "display(type_weight_df)\n",
    "\n",
    "print(\"Queue Class Weights\")\n",
    "display(queue_weight_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ee5b37",
   "metadata": {},
   "source": [
    "# **`embedding`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d882e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "print(\"library done\")\n",
    "# Load model\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "print(\"load bert done\")\n",
    "bert_model.eval()\n",
    "print(\"load eval done\")\n",
    "# # Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_model.to(device)\n",
    "\n",
    "# Prepare batches\n",
    "batch_size = 16\n",
    "input_ids = X_train_tokenized[\"input_ids\"]\n",
    "attention_mask = X_train_tokenized[\"attention_mask\"]\n",
    "print(\"batching done\")\n",
    "cls_embeddings_list = []\n",
    "\n",
    "# Disable gradient computation and iterate with progress bar\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(\n",
    "        range(0, len(input_ids), batch_size), desc=\"Embedding texts with BERT\"\n",
    "    ):\n",
    "        batch_input_ids = input_ids[i : i + batch_size].to(device)\n",
    "        batch_attention_mask = attention_mask[i : i + batch_size].to(device)\n",
    "\n",
    "        outputs = bert_model(\n",
    "            input_ids=batch_input_ids, attention_mask=batch_attention_mask\n",
    "        )\n",
    "\n",
    "        # Take [CLS] token embedding\n",
    "        batch_cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu()\n",
    "        cls_embeddings_list.append(batch_cls_embeddings)\n",
    "\n",
    "# Concatenate all embeddings\n",
    "cls_embeddings = torch.cat(cls_embeddings_list, dim=0)\n",
    "filename = \"cls_embeddings.pt\"\n",
    "torch.save(cls_embeddings, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c8e740",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"cls_embeddings.pt\"\n",
    "loaded_cls_embeddings = torch.load(filename)\n",
    "print(f\"Embeddings successfully loaded. Shape: {loaded_cls_embeddings.shape}\")\n",
    "\n",
    "# Build DataFrame\n",
    "train_embeddings_df = pd.DataFrame(cls_embeddings.numpy())\n",
    "train_embeddings_df[\"priority\"] = y_train_priority.numpy()\n",
    "train_embeddings_df[\"type\"] = y_train_type.numpy()\n",
    "train_embeddings_df[\"queue\"] = y_train_queue.numpy()\n",
    "\n",
    "train_embeddings_df.value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321120a6",
   "metadata": {},
   "source": [
    "# **Fine-Tuning**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b228c5",
   "metadata": {},
   "source": [
    "## **Load Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e954c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "class MultiOutputBERT(nn.Module):\n",
    "    def __init__(\n",
    "        self, model_name, num_priority_classes, num_type_classes, num_queue_classes\n",
    "    ):\n",
    "        super(MultiOutputBERT, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.1)  # static dropout for baseline\n",
    "\n",
    "        # Each output head corresponds to one classification target\n",
    "        self.priority_head = nn.Linear(\n",
    "            self.bert.config.hidden_size, num_priority_classes\n",
    "        )\n",
    "        self.type_head = nn.Linear(self.bert.config.hidden_size, num_type_classes)\n",
    "        self.queue_head = nn.Linear(self.bert.config.hidden_size, num_queue_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        cls_output = self.dropout(cls_output)\n",
    "\n",
    "        priority_logits = self.priority_head(cls_output)\n",
    "        type_logits = self.type_head(cls_output)\n",
    "        queue_logits = self.queue_head(cls_output)\n",
    "\n",
    "        return priority_logits, type_logits, queue_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af7c0f4",
   "metadata": {},
   "source": [
    "## **Train loop**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d90e85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Static hyperparameters\n",
    "LEARNING_RATE = 2e-5\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 16\n",
    "DROPOUT = 0.1\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Initialize model\n",
    "num_priority_classes = len(priority_classes)\n",
    "num_type_classes = len(type_classes)\n",
    "num_queue_classes = len(queue_classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MultiOutputBERT(\n",
    "    model_name=\"bert-base-cased\",\n",
    "    num_priority_classes=num_priority_classes,\n",
    "    num_type_classes=num_type_classes,\n",
    "    num_queue_classes=num_queue_classes,\n",
    ").to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Loss functions (weighted for imbalance)\n",
    "type_weights = torch.tensor(type_weights, dtype=torch.float32).to(device)\n",
    "queue_weights = torch.tensor(queue_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# load torch weights\n",
    "type_weights = torch.load(\"type_weights.pt\")\n",
    "queue_weights = torch.load(\"queue_weights.pt\")\n",
    "\n",
    "criterion_priority = nn.CrossEntropyLoss()\n",
    "criterion_type = nn.CrossEntropyLoss(weight=type_weights.to(device))\n",
    "criterion_queue = nn.CrossEntropyLoss(weight=queue_weights.to(device))\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    X_train_tokenized[\"input_ids\"],\n",
    "    X_train_tokenized[\"attention_mask\"],\n",
    "    y_train_priority.long(),\n",
    "    y_train_type.long(),\n",
    "    y_train_queue.long(),\n",
    ")\n",
    "\n",
    "val_dataset = TensorDataset(\n",
    "    X_val_tokenized[\"input_ids\"],\n",
    "    X_val_tokenized[\"attention_mask\"],\n",
    "    y_val_priority.long(),\n",
    "    y_val_type.long(),\n",
    "    y_val_queue.long(),\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd8e21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import trainer\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n===== Epoch {epoch+1}/{EPOCHS} =====\")\n",
    "\n",
    "    # ---- Training Phase ----\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    train_progress = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for batch in train_progress:\n",
    "        batch = [item.to(device) for item in batch]\n",
    "        input_ids, attention_mask, priority, type_, queue = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits_priority, logits_type, logits_queue = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = (\n",
    "            criterion_priority(logits_priority, priority)\n",
    "            + criterion_type(logits_type, type_)\n",
    "            + criterion_queue(logits_queue, queue)\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        train_progress.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(\"train done\")\n",
    "\n",
    "    # eval loop\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    print(\"load val\")\n",
    "    val_progress = tqdm(val_loader, desc=\"Validating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in val_progress:\n",
    "            batch = [item.to(device) for item in batch]\n",
    "            input_ids, attention_mask, priority, type_, queue = batch\n",
    "\n",
    "            logits_priority, logits_type, logits_queue = model(\n",
    "                input_ids, attention_mask\n",
    "            )\n",
    "            val_loss = (\n",
    "                criterion_priority(logits_priority, priority)\n",
    "                + criterion_type(logits_type, type_)\n",
    "                + criterion_queue(logits_queue, queue)\n",
    "            )\n",
    "\n",
    "            total_val_loss += val_loss.item()\n",
    "            val_progress.set_postfix(val_loss=val_loss.item())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f} | Val Loss = {avg_val_loss:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2b2d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Plot Learning Curve\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_losses, label=\"Training Loss\", marker=\"o\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\", marker=\"s\")\n",
    "plt.title(\"Learning Curve - Multi-Output BERT\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ec09e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
